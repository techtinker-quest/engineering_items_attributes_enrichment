"""
Base provider interface for LLM integrations.

This module defines the abstract base class and data structures for integrating
multiple LLM providers (OpenAI, Anthropic, Google) with standardized interfaces.
All concrete provider implementations must inherit from LLMProvider and implement
the required abstract methods.

Classes:
    LLMProvider: Abstract base class for LLM providers.
    LLMResponse: Standardized response container for LLM API calls.
    TokenUsage: Token usage tracking for cost calculation.
    ErrorDetails: Structured error information.
    ModelInfo: Model capabilities and pricing information.

Exceptions:
    LLMProviderError: Base exception for all LLM provider errors.
    AuthenticationError: Invalid or expired credentials.
    RateLimitError: Provider rate limit exceeded.
    TimeoutError: Request timeout.
    APIError: General API error.
    ModelNotFoundError: Requested model not available.
    InvalidInputError: Invalid input parameters.

Example:
    >>> class OpenAIProvider(LLMProvider):
    ...     def call(self, prompt, image, model, max_tokens, temperature, response_format, timeout):
    ...         # Implementation
    ...         pass
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Literal, Optional, Union


# ============================================================================
# Custom Exception Hierarchy
# ============================================================================


class LLMProviderError(Exception):
    """Base exception for all LLM provider errors.

    Attributes:
        message: Human-readable error message.
        provider: Provider name where error occurred.
        model: Model identifier if applicable.
        request_id: Optional request/trace ID for debugging.
    """

    def __init__(
        self,
        message: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        request_id: Optional[str] = None,
    ) -> None:
        super().__init__(message)
        self.message = message
        self.provider = provider
        self.model = model
        self.request_id = request_id


class AuthenticationError(LLMProviderError):
    """Raised when API credentials are invalid or expired."""

    pass


class RateLimitError(LLMProviderError):
    """Raised when provider rate limit is exceeded."""

    pass


class TimeoutError(LLMProviderError):
    """Raised when request exceeds timeout threshold."""

    pass


class APIError(LLMProviderError):
    """Raised for general API errors."""

    pass


class ModelNotFoundError(LLMProviderError):
    """Raised when requested model is not available."""

    pass


class InvalidInputError(LLMProviderError):
    """Raised when input parameters are invalid."""

    pass


# ============================================================================
# Data Structures
# ============================================================================


@dataclass(frozen=True)
class TokenUsage:
    """Token usage tracking for LLM API calls.

    Tracks input/output tokens and image count for cost calculation and budget monitoring.
    All token counts must be non-negative integers. Immutable for data integrity.

    Attributes:
        input_tokens: Number of tokens in the prompt/context sent to the LLM.
        output_tokens: Number of tokens generated by the LLM in the response.
        image_count: Number of images included in the request (for vision models).
            Defaults to 0 for text-only requests.

    Raises:
        ValueError: If any token count is negative.

    Example:
        >>> usage = TokenUsage(input_tokens=500, output_tokens=150, image_count=2)
        >>> total_tokens = usage.input_tokens + usage.output_tokens
    """

    input_tokens: int
    output_tokens: int
    image_count: int = 0

    def __post_init__(self) -> None:
        """Validate token counts are non-negative."""
        if self.input_tokens < 0:
            raise ValueError(
                f"input_tokens must be non-negative, got {self.input_tokens}"
            )
        if self.output_tokens < 0:
            raise ValueError(
                f"output_tokens must be non-negative, got {self.output_tokens}"
            )
        if self.image_count < 0:
            raise ValueError(
                f"image_count must be non-negative, got {self.image_count}"
            )

    def to_dict(self) -> dict[str, int]:
        """Convert to dictionary for serialization.

        Returns:
            dict[str, int]: Dictionary representation.
        """
        return {
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "image_count": self.image_count,
        }

    @classmethod
    def from_dict(cls, data: dict[str, int]) -> TokenUsage:
        """Create TokenUsage from dictionary.

        Args:
            data: Dictionary with token counts.

        Returns:
            TokenUsage: New instance.
        """
        return cls(
            input_tokens=data["input_tokens"],
            output_tokens=data["output_tokens"],
            image_count=data.get("image_count", 0),
        )


@dataclass(frozen=True)
class ErrorDetails:
    """Structured error information for failed LLM calls.

    Attributes:
        error_code: Machine-readable error code (e.g., 'rate_limit', 'invalid_key').
        message: Human-readable error message.
        details: Additional provider-specific error information.
    """

    error_code: str
    message: str
    details: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization.

        Returns:
            dict[str, Any]: Dictionary representation.
        """
        return {
            "error_code": self.error_code,
            "message": self.message,
            "details": self.details,
        }


@dataclass(frozen=True)
class LLMResponse:
    """Standardized LLM response container.

    Encapsulates the response from any LLM provider with metadata for tracking,
    debugging, and cost calculation. The `success` flag gates downstream processing.
    Immutable for data integrity.

    Attributes:
        content: The generated text response from the LLM. Empty string if call failed.
        tokens_used: Token usage breakdown (input/output/images) for cost tracking.
        model_used: Exact model identifier used (may differ from requested if fallback occurred).
        provider: Provider name (e.g., 'openai', 'anthropic', 'google').
        success: True if API call completed successfully, False otherwise.
        cost_usd: Cost in USD for this API call.
        latency_ms: API response time in milliseconds.
        request_id: Optional request/trace ID for debugging.
        error_details: Structured error information if success=False, None otherwise.
        metadata: Additional provider-specific metadata (e.g., finish_reason).

    Example:
        >>> response = LLMResponse(
        ...     content="Extracted: Part #12345",
        ...     tokens_used=TokenUsage(input_tokens=150, output_tokens=20),
        ...     model_used="gpt-4o-mini",
        ...     provider="openai",
        ...     success=True,
        ...     cost_usd=0.0025,
        ...     latency_ms=234.5
        ... )
    """

    content: str
    tokens_used: TokenUsage
    model_used: str
    provider: str
    success: bool
    cost_usd: float
    latency_ms: float
    request_id: Optional[str] = None
    error_details: Optional[ErrorDetails] = None
    metadata: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization.

        Returns:
            dict[str, Any]: Dictionary representation.
        """
        return {
            "content": self.content,
            "tokens_used": self.tokens_used.to_dict(),
            "model_used": self.model_used,
            "provider": self.provider,
            "success": self.success,
            "cost_usd": self.cost_usd,
            "latency_ms": self.latency_ms,
            "request_id": self.request_id,
            "error_details": (
                self.error_details.to_dict() if self.error_details else None
            ),
            "metadata": self.metadata,
        }


@dataclass(frozen=True)
class ModelInfo:
    """Model capabilities and pricing information.

    Attributes:
        name: Model identifier (e.g., 'gpt-4o', 'claude-3-opus-20240229').
        supports_vision: Whether model supports image inputs.
        max_input_tokens: Maximum input context window size.
        max_output_tokens: Maximum output tokens per request.
        input_cost_per_1m: Cost per 1M input tokens in USD.
        output_cost_per_1m: Cost per 1M output tokens in USD.
        image_cost: Optional cost per image in USD.
    """

    name: str
    supports_vision: bool
    max_input_tokens: int
    max_output_tokens: int
    input_cost_per_1m: float
    output_cost_per_1m: float
    image_cost: Optional[float] = None


# ============================================================================
# Abstract Base Class
# ============================================================================


class LLMProvider(ABC):
    """Abstract base class for LLM provider implementations.

    Defines the contract that all concrete LLM providers (OpenAI, Anthropic, Google)
    must implement. Ensures consistent interface for API calls, credential validation,
    and model availability checks across all providers.

    Concrete implementations must handle:
    - API authentication and credential management
    - Request/response serialization
    - Provider-specific error handling and retries
    - Rate limiting and timeout handling
    - Input validation and sanitization

    Example:
        >>> class CustomProvider(LLMProvider):
        ...     def call(self, prompt, image, model, max_tokens, temperature, response_format, timeout):
        ...         # Custom implementation
        ...         return LLMResponse(...)
        ...
        ...     def get_available_models(self):
        ...         return [ModelInfo(...), ...]
        ...
        ...     def validate_credentials(self):
        ...         return self._check_api_key()
    """

    @abstractmethod
    def call(
        self,
        prompt: str,
        image: Optional[Union[bytes, Path, str]] = None,
        model: str = "",
        max_tokens: int = 1000,
        temperature: float = 0.0,
        response_format: Optional[dict[str, Any]] = None,
        timeout: float = 30.0,
    ) -> LLMResponse:
        """Make API call to the LLM provider.

        Executes a single API request with the given parameters. Implementations must
        handle authentication, request formatting, response parsing, and error handling.

        Args:
            prompt: The text prompt to send to the LLM. Must not be empty or whitespace.
            image: Optional image for vision models. Can be:
                - bytes: Raw image data (PNG/JPEG)
                - Path: File path to image
                - str: URL or file path string
                None for text-only requests.
            model: Model identifier string (e.g., 'gpt-4o', 'claude-3-opus-20240229').
                Must be a valid model for this provider.
            max_tokens: Maximum number of tokens to generate in the response.
                Must be positive and within provider's model limits.
            temperature: Sampling temperature (0.0 = deterministic, higher = creative).
                Valid range depends on provider (typically [0.0, 2.0]).
            response_format: Optional response format specification. Provider-specific
                structure (e.g., {"type": "json_object"} for OpenAI JSON mode).
            timeout: Request timeout in seconds. Defaults to 30.0 seconds.

        Returns:
            LLMResponse: Standardized response with content, token usage, cost, and metadata.
                Returns success=False with error_details if API call fails.

        Raises:
            InvalidInputError: If parameters are invalid (empty prompt, negative max_tokens, etc.).
            AuthenticationError: If API credentials are invalid or expired.
            RateLimitError: If provider rate limit is exceeded.
            TimeoutError: If request exceeds timeout threshold.
            ModelNotFoundError: If requested model is not available.
            APIError: For other provider-specific API errors.

        Example:
            >>> provider = OpenAIProvider(api_key="sk-...")
            >>> response = provider.call(
            ...     prompt="Extract part number from this drawing",
            ...     image=Path("drawing.png"),
            ...     model="gpt-4o",
            ...     max_tokens=500,
            ...     temperature=0.0,
            ...     timeout=60.0
            ... )
            >>> if response.success:
            ...     print(f"Cost: ${response.cost_usd:.4f}")
            ...     print(response.content)
        """
        pass

    @abstractmethod
    def get_available_models(self) -> list[ModelInfo]:
        """Get list of available models with capabilities and pricing.

        Returns model information including vision support, token limits, and pricing.
        Used for validation, dynamic model selection, and cost estimation.

        Returns:
            list[ModelInfo]: List of model information objects supported by this provider.
                Empty list if no models are available or credentials are invalid.

        Example:
            >>> provider = OpenAIProvider(api_key="sk-...")
            >>> models = provider.get_available_models()
            >>> vision_models = [m for m in models if m.supports_vision]
            >>> print(f"Vision models: {[m.name for m in vision_models]}")
        """
        pass

    @abstractmethod
    def validate_credentials(self) -> bool:
        """Validate API credentials for this provider.

        Checks if the configured API key/credentials are valid by making a lightweight
        API call. Should be called during initialization to fail fast if credentials
        are invalid. Returns False on failure rather than raising exceptions.

        Returns:
            bool: True if credentials are valid and API is accessible, False otherwise.

        Example:
            >>> provider = AnthropicProvider(api_key="sk-ant-...")
            >>> if not provider.validate_credentials():
            ...     raise ConfigurationError("Invalid Anthropic API key")
        """
        pass

    @abstractmethod
    def calculate_cost(self, tokens_used: TokenUsage, model: str) -> float:
        """Calculate cost for API call based on token usage.

        Implements provider-specific pricing logic. Costs vary by model tier
        and whether images are included in the request.

        Args:
            tokens_used: Token usage breakdown from the API call.
            model: Model identifier used for the call.

        Returns:
            float: Cost in USD for this API call.

        Raises:
            ModelNotFoundError: If model is not recognized by this provider.

        Example:
            >>> provider = OpenAIProvider(api_key="sk-...")
            >>> usage = TokenUsage(input_tokens=1000, output_tokens=500, image_count=1)
            >>> cost = provider.calculate_cost(usage, "gpt-4o")
            >>> print(f"Cost: ${cost:.4f}")
        """
        pass

    def get_provider_name(self) -> str:
        """Get the name of this provider.

        Returns:
            str: Provider name (e.g., 'openai', 'anthropic', 'google').
        """
        return self.__class__.__name__.replace("Provider", "").lower()
